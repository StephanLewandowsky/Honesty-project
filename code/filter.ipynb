{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the environment and import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from spacy.tokens import DocBin\n",
    "import pandas as pd\n",
    "import key_dict as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "matcher = Matcher(nlp.vocab)\n",
    "phraseMatcher = PhraseMatcher(nlp.vocab, attr=\"LEMMA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Tweets\\sampled_tweets.xlsx\")\n",
    "data = df[df[\"retweeted\"] == False] # to exclude retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose units to collect (sentence only or whole documents):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # to collect sentence only\n",
    "matched_sents = [] \n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]  # Matched span\n",
    "    sent = span.sent  # Sentence containing matched span\n",
    "    matched_sents.append(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # to collect entire doc/cell\n",
    "matched_docs = [] \n",
    "\n",
    "def collect_docs(matcher, doc, i, matches):\n",
    "    matched_docs.append(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import selected patterns from keywords_and_patterns.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the patterns to add to the matcher\n",
    "matcher.add(\"VERBS\", k.belief_patterns[\"verbs\"], on_match=collect_docs)\n",
    "matcher.add(\"NOUNS\", k.belief_patterns[\"nouns\"], on_match=collect_docs)\n",
    "matcher.add(\"ADVERBS\", k.belief_patterns[\"adverbs\"], on_match=collect_docs)\n",
    "matcher.add(\"ADJECTIVES\", k.belief_patterns[\"adjectives\"], on_match=collect_docs)\n",
    "matcher.add(\"SURE\", k.belief_patterns[\"sure\"], on_match=collect_docs)\n",
    "phraseMatcher.add(\"PHRASES\", k.belief_patterns[\"phrases\"], on_match=collect_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the `doc` object on the dataset. This should only be done when the dataset changes as it can take time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = data[\"text\"].apply(lambda x: nlp(str(x)))\n",
    "doc.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the `doc` object has been created, save it to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docbin = DocBin(docs=doc)\n",
    "docbin.__len__()\n",
    "docbin.to_disk(path=\"docbin.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load saved `doc`. Make sure the vocabulary used to create the `doc` matches the loaded vocabulary (see `nlp = spacy.load()`).  Then, convert it to a list to make it iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docbin_loaded = DocBin().from_disk(path=\"docbin.spacy\")\n",
    "docbin_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_loaded = list(docbin_loaded.get_docs(nlp.vocab))\n",
    "docs_loaded.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the matchers on the single docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in docs_loaded:\n",
    "    matcher(x)\n",
    "    phraseMatcher(x)\n",
    "len(matched_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete duplicates (docs that satisfy more than one matcher):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [i for n, i in enumerate(matched_docs) if i not in matched_docs[:n]]\n",
    "len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose what to output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save texts only\n",
    "pd.DataFrame(output).to_excel(\"output.xlsx\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save entire rows (this includes Twitter metadata)\n",
    "boolean_series = df.text.isin(output)\n",
    "filtered_df = df[boolean_series]\n",
    "pd.DataFrame(filtered_df).to_excel(\"output.xlsx\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "719f56f5296a31fd7398b3f43dd2caf4b08de37b971ef334119ac0467b0e656a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
