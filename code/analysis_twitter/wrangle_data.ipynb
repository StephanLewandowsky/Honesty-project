{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d70086-b0cd-471d-867d-21e782483b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../../utilities/twitter_functions')\n",
    "import twitter_functions as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139da757-f9f3-4839-b4e8-224adf5dafd7",
   "metadata": {},
   "source": [
    "# Create a URL data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bc1b3-3813-489c-8aa9-266ddf21b8fe",
   "metadata": {},
   "source": [
    "## Expand URL lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d47cd12-d37e-48b2-9c2a-762a6cedc62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_360777/3303023762.py:4: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tweets = pd.read_csv(join(src, fname),\n"
     ]
    }
   ],
   "source": [
    "# load the cleaned timeline-data\n",
    "src = \"../../data/twitter\"\n",
    "fname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16_clean.csv.gzip\"\n",
    "tweets = pd.read_csv(join(src, fname),\n",
    "                 compression=\"gzip\",\n",
    "                 usecols=[\"id\", \"author_id\", \"created_at\", \"expanded_urls\",\n",
    "                          \"retweeted\", \"quoted\", \"reply\"])\n",
    "tweets = tweets.drop_duplicates(subset=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ffcd134-eaad-4cc9-b662-6f0067f2d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the URL lists\n",
    "tweets[\"expanded_urls\"] = tweets[\"expanded_urls\"].fillna(\"[]\")\n",
    "tweets[\"expanded_urls\"] = tweets[\"expanded_urls\"].apply(lambda x: eval(x))\n",
    "tweets[\"has_url\"] = tweets[\"expanded_urls\"].apply(lambda x: len(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d62d4367-4074-48f6-99c6-f0c8815024cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_360777/616169884.py:6: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  urls = tweets['expanded_urls']\\\n"
     ]
    }
   ],
   "source": [
    "# expand the url lists such that tweets with a list of N urls are converted\n",
    "# into N individual rows, one for each URL\n",
    "# NOTE: this operation takes a substantial amount of time, which is why we\n",
    "# save the outcome so we can skip this step if we want to re-do other parts of\n",
    "# the data wrangling later\n",
    "urls = tweets['expanded_urls']\\\n",
    "    .apply(pd.Series)\\\n",
    "    .reset_index()\\\n",
    "    .melt(id_vars='index')\\\n",
    "    .dropna()[['index', 'value']]\\\n",
    "    .set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c783255-9136-4a3d-a00c-d20c7838c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the expanded URL data frame with the tweet data frame\n",
    "urls = pd.merge(\n",
    "    urls,\n",
    "    tweets[['id']],\n",
    "    left_index=True,\n",
    "    right_index=True).rename(columns={'value_x': 'expanded_urls'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3a37e86-396c-4706-ac02-0b3f4f2c5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some tweets contain the same URL twice. We drop these\n",
    "urls = urls.drop_duplicates(subset=[\"id\", \"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55ba76e8-09a0-4f2e-bd7c-198a30a14e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = pd.merge(tweets, urls, left_on=\"id\", right_on=\"id\", how=\"left\")\n",
    "del tweets\n",
    "\n",
    "urls[\"N_urls\"] = urls[\"expanded_urls\"].apply(lambda x: len(x))\n",
    "urls = urls.rename(columns={\"value\":\"url\"})\n",
    "\n",
    "# save the outcome\n",
    "urls.to_csv(join(src, \"combined_US_politician_twitter_timelines_2010-11-06_to_2021-03-16_clean_urls.csv.xz\"),\n",
    "          compression=\"xz\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bb6ab2-cfc2-4eca-8848-46d04719fc70",
   "metadata": {},
   "source": [
    "## Add tweet metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9287984-bfed-41cd-b853-226df27367c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data frame with the expanded URLs\n",
    "src = \"../../data/twitter\"\n",
    "fname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2021-03-16_clean_urls.csv.xz\"\n",
    "urls = pd.read_csv(join(src, fname), compression=\"xz\", parse_dates=[\"created_at\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07371d18-7280-4dc1-9723-fc500507ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the public metrics information for the collected tweets\n",
    "src = \"../../data/twitter\"\n",
    "fname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16_clean.csv.gzip\"\n",
    "tweet_metrics = pd.read_csv(join(src, fname),\n",
    "                 compression=\"gzip\",\n",
    "                 usecols=[\"id\", \"retweet_count\",\n",
    "                          \"reply_count\", \"like_count\", \"quote_count\"])\n",
    "tweet_metrics = tweet_metrics.drop_duplicates(subset=\"id\")\n",
    "# merge the tweet metrics with the tweet data frame\n",
    "urls = pd.merge(urls, tweet_metrics, how=\"left\", left_on=\"id\", right_on=\"id\")\n",
    "del tweet_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "865d2b0c-9f9f-4b5b-8dd2-999de72b5f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the additional quote characters from the tweet ID and author ID columns\n",
    "# that prevent parsing of these fields as numbers\n",
    "urls[\"id\"] = urls[\"id\"].apply(lambda x: x.replace('\"', ''))\n",
    "urls[\"author_id\"] = urls[\"author_id\"].apply(lambda x: x.replace('\"', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d582d2-3902-4505-b3f2-912132db85ba",
   "metadata": {},
   "source": [
    "## Add unraveled URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6064772-315a-4480-a4b7-6819938ec19b",
   "metadata": {},
   "source": [
    "Note: run the following to unravel a list of URLs:  \n",
    "`python ../../../utilities/unravel_urls/unravel_urls.py url_list.csv.gzip -dst unraveled_urls3/ -v 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0a307d6-8c0f-4f81-943f-2224dde5809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the list of originally shortened URLs with their expansions to their true\n",
    "# destination\n",
    "src = \"../../data/twitter\"\n",
    "fname = \"unraveled_urls.csv.xz\"\n",
    "unraveled_urls = pd.read_csv(join(src, fname), compression=\"xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99817340-0e3c-4ff3-bf3f-39e28d4c712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add URL information\n",
    "urls = pd.merge(urls, unraveled_urls, left_on=\"url\", right_on=\"url\", how=\"left\")\n",
    "\n",
    "# add indicator of whether the URL was originally shortened\n",
    "urls[\"shortened_url\"] = False\n",
    "urls.loc[urls[\"unraveled_url\"].dropna().index, \"shortened_url\"] = True\n",
    "\n",
    "# replace the shortened URL with the unraveled URL\n",
    "urls.loc[urls[\"unraveled_url\"].dropna().index, \"url\"] = \\\n",
    "    urls.loc[urls[\"unraveled_url\"].dropna().index, \"unraveled_url\"]\n",
    "urls = urls.drop(columns=[\"unraveled_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eb618ca-2d99-4b12-b55d-34cba20edadf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found malformed URL https\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n",
      "found malformed URL http\n"
     ]
    }
   ],
   "source": [
    "# extract the domain from the URL\n",
    "urls[\"domain\"] = urls[\"url\"].apply(tf.extract_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beecaea6-0097-46bd-81ba-447d54aedbe7",
   "metadata": {},
   "source": [
    "## Add NewsGuard nutrition scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bd17d-6a55-4776-bb34-082cf521baf6",
   "metadata": {},
   "source": [
    "Newsguard rating cutoff: 60 (see [description](https://www.newsguardtech.com/ratings/rating-process-criteria/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e27eb7a1-a3c0-4b14-8cd3-3b003a6d0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the nutrition labels\n",
    "src = \"../../data/newsguard/newsguard_2022-03/03\"\n",
    "fname = \"metadata-2022030100.csv\"\n",
    "NG_scores = pd.read_csv(join(src, fname))\n",
    "# if more than one score exists for the same domain, keep the most recent one\n",
    "NG_scores = NG_scores.sort_values(by=[\"Domain\",\"Last Updated\"], ascending=False)\n",
    "NG_scores = NG_scores.drop_duplicates(subset=[\"Domain\"])\n",
    "NG_scores = NG_scores.rename(columns={\"Domain\":\"domain\"})\n",
    "\n",
    "# threshold scores at various cutoffs to define untrustworthy domains\n",
    "NG_scores[\"fishy_60\"] = 0\n",
    "NG_scores[\"fishy_40\"] = 0\n",
    "NG_scores[\"fishy_20\"] = 0\n",
    "NG_scores.loc[NG_scores[NG_scores[\"Score\"] < 60].index, \"fishy_60\"] = 1\n",
    "fishy_60_domains = set(NG_scores[NG_scores[\"fishy_60\"] == 1][\"domain\"])\n",
    "NG_scores.loc[NG_scores[NG_scores[\"Score\"] < 40].index, \"fishy_40\"] = 1\n",
    "fishy_40_domains = set(NG_scores[NG_scores[\"fishy_40\"] == 1][\"domain\"])\n",
    "NG_scores.loc[NG_scores[NG_scores[\"Score\"] < 20].index, \"fishy_20\"] = 1\n",
    "fishy_20_domains = set(NG_scores[NG_scores[\"fishy_20\"] == 1][\"domain\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9db24e87-3f41-41b1-9fee-5ab71ddc7bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trustworthy EN: 62.755\n",
      "Trustworthy DE: 74.483\n"
     ]
    }
   ],
   "source": [
    "all_EN = len(NG_scores[(NG_scores[\"Language\"] == \"en\")])\n",
    "untrustworthy_EN = len(NG_scores[(NG_scores[\"Language\"] == \"en\") & (NG_scores[\"Score\"] < 60)])\n",
    "print(f\"Trustworthy EN: {100 - untrustworthy_EN/all_EN * 100:1.3f}\")\n",
    "\n",
    "all_DE = len(NG_scores[(NG_scores[\"Language\"] == \"de\")])\n",
    "untrustworthy_DE = len(NG_scores[(NG_scores[\"Language\"] == \"de\") & (NG_scores[\"Score\"] < 60)])\n",
    "print(f\"Trustworthy DE: {100 - untrustworthy_DE/all_DE * 100:1.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54dcc32a-051e-4f14-b427-2abd239582b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_cols = [\n",
    "    \"fishy_60\", \"fishy_40\", \"fishy_20\", \"domain\",\n",
    "    \"Rating\", \"Score\", \"Country\", \"Language\", \n",
    "    \"Does not repeatedly publish false content\",\n",
    "    \"Gathers and presents information responsibly\",\n",
    "    \"Regularly corrects or clarifies errors\",\n",
    "    \"Handles the difference between news and opinion responsibly\",\n",
    "    \"Avoids deceptive headlines\",\n",
    "    \"Website discloses ownership and financing\",\n",
    "    \"Clearly labels advertising\",\n",
    "    \"Reveals who's in charge, including any possible conflicts of interest\",\n",
    "    \"The site provides names of content creators, along with either contact or biographical information\"\n",
    "]\n",
    "nutrition_categories = {col:f\"C_{i}\" for i, col in enumerate(nutrition_cols[8:])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af936414-bcac-417d-a0ad-58e5aab40f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the nutrition information to the tweet data table\n",
    "urls = pd.merge(urls, NG_scores[nutrition_cols],\n",
    "         left_on=\"domain\", right_on=\"domain\", how=\"left\")\n",
    "urls = urls.rename(columns=nutrition_categories)\n",
    "del NG_scores\n",
    "\n",
    "# transform the labels into binary values\n",
    "for col in nutrition_categories.values():\n",
    "    urls[col] = urls[col].replace({\"Yes\":0, \"No\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddbcc04c-f946-4d8f-bace-70449904ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the list of all URLs with a NewsGuard score for text straping\n",
    "url_export = urls[[\"url\", \"Score\"]].copy()\n",
    "url_export = url_export.drop_duplicates()\n",
    "url_export[[\"url\", \"Score\"]]\\\n",
    "    .rename(columns={\"Score\":\"score\"})\\\n",
    "    .to_csv(join(src, \"unraveled_url_list.csv.gzip\"), \n",
    "            index=False, compression=\"gzip\")\n",
    "del url_export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc4e71e-2b3e-4443-a9a7-aef9deef614a",
   "metadata": {},
   "source": [
    "## Add alternative trustworthiness labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28a4a1fe-f8c2-4dd9-a93d-db0c7cadc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the list of independently compiled trustworthiness labels for \n",
    "# news sources\n",
    "src = \"../../data/twitter\"\n",
    "fname = \"unstrustworthy_domain_list.csv\"\n",
    "alt_labels = pd.read_csv(join(src, fname))\n",
    "alt_labels = alt_labels.rename(columns = {\"type\":\"unreliable\", \"url\":\"Domain\"})\n",
    "\n",
    "# convert reliability labels to binary\n",
    "alt_labels[\"unreliable\"] = alt_labels[\"unreliable\"]\\\n",
    "    .replace({\"reliable\":0, \"unreliable\":1})\n",
    "\n",
    "# merge with the tweet data table\n",
    "urls = pd.merge(urls, alt_labels[[\"accuracy\", \"transparency\", \n",
    "        \"unreliable\", \"Domain\"]], how=\"left\", left_on=\"domain\",\n",
    "         right_on=\"Domain\")\n",
    "del alt_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae920e-36fa-413c-b46c-e005aa0c64b4",
   "metadata": {},
   "source": [
    "## Add truth seeking & belief speaking scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64e37aa4-cee5-49b6-afe4-0907dee81c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receiving incremental file list\n",
      "\n",
      "sent 20 bytes  received 139 bytes  106.00 bytes/sec\n",
      "total size is 666,404,969  speedup is 4,191,226.22\n"
     ]
    }
   ],
   "source": [
    "! rsync -avze ssh jlasser@medea:/data/honesty/corpora/Twitter/combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16_threshold_label.csv ../../data/twitter/combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16_threshold_label.csv --progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdaf3065-2c0d-4d15-9c7a-7767ac5cb0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the word matching counts for belief-speaking and truth-seeking\n",
    "src = \"../../data/twitter\"\n",
    "fname = \"combined_US_politician_twitter_timelines_2010-11-06_to_2022-03-16_threshold_label.csv\"\n",
    "cols = [\"id\", \"author_id\", \"belief_count\", \"truth_count\", \"created_at\"]\n",
    "honesty_tweets = pd.read_csv(join(src, fname), usecols=cols, parse_dates=[\"created_at\"])\n",
    "honesty_tweets[\"author_id\"] = honesty_tweets[\"author_id\"].apply(lambda x: x.replace('\"', ''))\n",
    "honesty_tweets[\"id\"] = honesty_tweets[\"id\"].apply(lambda x: x.replace('\"', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ae406b9-1806-4509-af1b-fc439d47d681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# belief-speaking and truth-seeking labels for each tweet are assigned based on\n",
    "# the majority of words matching to one of the two components. If there is a \n",
    "# tie, the tweet is assigned to both components this results in\n",
    "# 190650 unambiguous belief tweets\n",
    "# 240302 unambiguous truth tweets\n",
    "# 30613 ties with count > 0 including 607 ties with count > 1 and 13 ties with count > 2\n",
    "\n",
    "honesty_tweets[\"belief\"] = 0\n",
    "honesty_tweets[\"truth\"] = 0\n",
    "honesty_tweets[\"neutral\"] = 0\n",
    "# unambigous majority votes\n",
    "honesty_tweets.loc[honesty_tweets[honesty_tweets[\"belief_count\"] > \\\n",
    "                    honesty_tweets[\"truth_count\"]].index, \"belief\"] = 1\n",
    "honesty_tweets.loc[honesty_tweets[honesty_tweets[\"truth_count\"] > \\\n",
    "                    honesty_tweets[\"belief_count\"]].index, \"truth\"] = 1\n",
    "\n",
    "# ties\n",
    "honesty_tweets.loc[honesty_tweets[(honesty_tweets[\"truth_count\"] == \\\n",
    "                    honesty_tweets[\"belief_count\"]) & \\\n",
    "                    (honesty_tweets[\"truth_count\"] > 0)].index, \"truth\"] = 1\n",
    "honesty_tweets.loc[honesty_tweets[(honesty_tweets[\"truth_count\"] == \\\n",
    "                    honesty_tweets[\"belief_count\"]) &\\\n",
    "                    (honesty_tweets[\"truth_count\"] > 0)].index, \"belief\"] = 1\n",
    "\n",
    "# neutral\n",
    "honesty_tweets.loc[honesty_tweets[(honesty_tweets[\"truth_count\"] == 0) & \\\n",
    "                    (honesty_tweets[\"belief_count\"] == 0)].index, \"neutral\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1c321c2-4fbf-4fbc-9f00-734c42db8838",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = pd.merge(honesty_tweets[[\"id\", \"belief\", \"truth\", \"neutral\"]], \n",
    "         urls, how=\"right\", left_on=\"id\", right_on=\"id\")\n",
    "del honesty_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615e8610-dd79-4384-a4c9-962ab6628cf1",
   "metadata": {},
   "source": [
    "## Add party affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "361cf5ea-2546-4825-993a-8368b1517ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# author IDs have become converted to floats / integers and lost the last 4\n",
    "# digits. We have a list of the correct IDs stored as strings and use it to\n",
    "# match the corrupted IDs back to the correct ones based on the first 13 digits\n",
    "# of the IDs\n",
    "\n",
    "ids = np.loadtxt(join(src, \"correct_author_ids.txt\"))\n",
    "ids = pd.DataFrame({\"id\":ids}).sort_values(by=\"id\")\n",
    "ids[\"id\"] = ids[\"id\"].astype(int).astype(str)\n",
    "partial_ids = ids[ids[\"id\"].apply(lambda x: len(x) >=14)].copy()\n",
    "partial_ids[\"id_part\"] = partial_ids[\"id\"].apply(lambda x: x[0:13])\n",
    "partial_ids = {row[\"id_part\"]:row[\"id\"] for i, row in partial_ids.iterrows()}\n",
    "\n",
    "def match_id(old_id):\n",
    "    if len(old_id) > 16:\n",
    "        id_part = old_id[0:13]\n",
    "        if id_part in partial_ids.keys():\n",
    "            correct_id = partial_ids[id_part]\n",
    "            return correct_id\n",
    "    return old_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68f1fdc1-5e0a-4834-9c62-9b94ebfa142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load party affiliation, strip \" used to ensure author_ids are stored as\n",
    "# strings and not numbers\n",
    "party_affiliation = pd.read_csv(join(src, \"party_affiliations_complete.csv\"))\n",
    "party_affiliation[\"author_id\"] = party_affiliation[\"author_id\"]\\\n",
    "    .apply(lambda x: x.replace('\"', ''))\n",
    "\n",
    "party_affiliation[\"author_id\"] = party_affiliation[\"author_id\"].apply(match_id)\n",
    "urls[\"author_id\"] = urls[\"author_id\"].apply(match_id)\n",
    "\n",
    "# merge fishy link information and information about party affiliation\n",
    "urls = pd.merge(urls, party_affiliation, how=\"left\", left_on=\"author_id\",\n",
    "    right_on=\"author_id\")\n",
    "del party_affiliation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854407a9-c578-4442-a0de-b671232aaa3a",
   "metadata": {},
   "source": [
    "# Create a tweet data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58b04781-9dad-4f95-bcbf-67ae7fedee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the current \"url\" data frame contains one row per URL, i.e. the same\n",
    "# tweet can be present more than once. To calculate the share of tweets with\n",
    "# unreliable information, we first calculate the mean NewsGuard score (and \n",
    "# mean accuracy and transparency) per tweet by averaging over all scores \n",
    "# of URLs that are present in a given tweet and then assigning \"fishy\" and\n",
    "# \"unreliable\" labels on the tweet level\n",
    "\n",
    "# columns that are defined on the tweet level\n",
    "tweet_cols = [\"id\", \"belief\", \"truth\", \"neutral\", \"author_id\",\n",
    "              \"created_at\", \"retweeted\", \"quoted\", \"reply\", \"has_url\",\n",
    "              \"retweet_count\", \"reply_count\", \"like_count\", \"quote_count\",\n",
    "              \"handle\", \"name\", \"party\"]\n",
    "tweets = urls[tweet_cols].drop_duplicates(subset=[\"id\"]).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a6213b-1870-4d10-b0eb-e29437d76fde",
   "metadata": {},
   "source": [
    "## Calculate average NewsGuard score and misinfo components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0fc3332-0793-444f-8764-ae7578b6b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "NewsGuard_categories = [\"C_0\", \"C_1\", \"C_2\", \"C_3\", \"C_4\", \"C_5\", \"C_6\", \"C_7\", \"C_8\"]\n",
    "average_scores = urls[[\"id\", \"Score\", \"transparency\", \"accuracy\"] + NewsGuard_categories]\\\n",
    "    .groupby(\"id\")\\\n",
    "    .agg(\"mean\")\n",
    "\n",
    "for cutoff in [20, 40, 60]:\n",
    "    average_scores[f\"fishy_{cutoff}\"] = np.nan\n",
    "    average_scores.loc[average_scores[\\\n",
    "                average_scores[\"Score\"] < cutoff].index, f\"fishy_{cutoff}\"] = 1\n",
    "    average_scores.loc[average_scores[\\\n",
    "                average_scores[\"Score\"] >= cutoff].index, f\"fishy_{cutoff}\"] = 0\n",
    "\n",
    "# round() rounds 0.5 down to 1. This behaviour is intended: if a Tweet\n",
    "# contains two URLs, one which conforms to a category, and one which doesn't,\n",
    "# we want to label the full tweet as not conforming to the category\n",
    "def nan_round(entry):\n",
    "    if entry != entry: return np.nan\n",
    "    else: return round(entry)\n",
    "for cat in NewsGuard_categories:\n",
    "    average_scores[cat] = average_scores[cat].apply(nan_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b490d1c0-59ab-4342-922c-fce0572d5d5e",
   "metadata": {},
   "source": [
    "## Calculate average accuracy & transparency score and unreliable domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f70b5d3-ce28-4609-a8de-52feadeeaefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores[\"unreliable\"] = np.nan\n",
    "# original definition: sources with transparency = 1 are unreliable\n",
    "# since transparency can have non-integer values after averaging, we decide\n",
    "# to label tweets with an average domain transparency value of links of\n",
    "# <= 1.5 as \"unreliable\", since that means that the majority of domains \n",
    "# linked to in the tweet are unreliable. If one domain with transparency 1\n",
    "# and one domain with transparency 2 are linked, the tweet is unreliable\n",
    "average_scores.loc[average_scores[\\\n",
    "            average_scores[\"transparency\"] <= 1.5].index, \"unreliable\"] = 1\n",
    "average_scores.loc[average_scores[\\\n",
    "            average_scores[\"transparency\"] > 1.5].index, \"unreliable\"] = 0\n",
    "# original defintion: sources with accuracy = 1 or 2 are unreliable\n",
    "# since accuracy can have non-integer values after averaging, we decide to\n",
    "# label tweets with an average domain accuracy value of links of <= 2.5 as\n",
    "# \"unreliable\", since that means that the majority of domains linked to in \n",
    "# the tweet are unreliable. If one domain with accuracy 2 and one domain \n",
    "# with accuracy 3 are linked, the tweet is unreliable.\n",
    "average_scores.loc[average_scores[\\\n",
    "            average_scores[\"accuracy\"] <= 2.5].index, \"unreliable\"] = 1\n",
    "average_scores.loc[average_scores[\\\n",
    "            average_scores[\"accuracy\"] > 2.5].index, \"unreliable\"] = 0\n",
    "\n",
    "tweets = pd.merge(tweets, average_scores, how=\"left\", left_on=\"id\", right_on=\"id\")\n",
    "del average_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcce382-f6bf-4ad7-9b57-08ef20082a65",
   "metadata": {},
   "source": [
    "# Create a user data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc2de824-8f53-4876-abf6-01056ffdc0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = tweets[[\"author_id\", \"handle\", \"name\", \"party\", \"id\"]]\\\n",
    "    .groupby([\"author_id\", \"handle\", \"name\", \"party\"])\\\n",
    "    .agg(\"count\")\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={\"id\":\"N_tweets\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2d7728-ea89-497f-8f42-494a39b00db0",
   "metadata": {},
   "source": [
    "## Add account stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ff10072-7659-4a81-9d6e-fc47c2e7c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../data/twitter/US_politician_twitter_accounts/clean\"\n",
    "fname = \"congress-member-unique-twitter-accounts_114-117.csv\"\n",
    "cols = [\"followers_count\", \"following_count\", \"tweet_count\", \"created_at\", \n",
    "        \"id\"]\n",
    "account_stats = pd.read_csv(join(src, fname), parse_dates=[\"created_at\"],\n",
    "                            usecols=cols)\n",
    "# if there is more than one entry for the same account, keep the most recent one\n",
    "account_stats = account_stats\\\n",
    "    .sort_values(\"created_at\", ascending=False)\\\n",
    "    .drop_duplicates(subset=\"id\")\\\n",
    "    .rename(columns={\"id\":\"author_id\"})\n",
    "account_stats[\"author_id\"] = account_stats[\"author_id\"].astype(str).apply(match_id)\n",
    "\n",
    "users = pd.merge(users, account_stats, how=\"left\", left_on=\"author_id\", right_on=\"author_id\")\n",
    "del account_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261401b6-eb61-4495-9bdb-a12570397fa0",
   "metadata": {},
   "source": [
    "## Add Congress information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e69ec38a-a3d1-4452-b740-ab537fbe7159",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../data/twitter/US_politician_twitter_accounts/clean\"\n",
    "fname = \"congress-member-twitter-handles_114-117.csv\"\n",
    "congress_twitter_handles = pd.read_csv(join(src, fname))\n",
    "congress_twitter_handles = congress_twitter_handles\\\n",
    "    .sort_values(by=\"congress\", ascending=False)\\\n",
    "    .drop_duplicates(subset=\"handle\")\\\n",
    "    .reset_index(drop=True)\n",
    "\n",
    "users = pd.merge(users, congress_twitter_handles, how=\"left\", left_on=\"handle\", right_on=\"handle\")\n",
    "del congress_twitter_handles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11974ba2-c6c1-42fa-9d99-bc1f1b949fd6",
   "metadata": {},
   "source": [
    "## Add share of untrustworthy domains (NewsGuard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a056d98f-6208-40d2-bb48-68e7861ac7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>fishy_60_sum</th>\n",
       "      <th>fishy_60_count</th>\n",
       "      <th>fishy_40_sum</th>\n",
       "      <th>fishy_40_count</th>\n",
       "      <th>fishy_20_sum</th>\n",
       "      <th>fishy_20_count</th>\n",
       "      <th>C_0_sum</th>\n",
       "      <th>C_0_count</th>\n",
       "      <th>C_1_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>fishy_share_20</th>\n",
       "      <th>C_0_share</th>\n",
       "      <th>C_1_share</th>\n",
       "      <th>C_2_share</th>\n",
       "      <th>C_3_share</th>\n",
       "      <th>C_4_share</th>\n",
       "      <th>C_5_share</th>\n",
       "      <th>C_6_share</th>\n",
       "      <th>C_7_share</th>\n",
       "      <th>C_8_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1009269193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>221</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0.208145</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.330317</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.081448</td>\n",
       "      <td>0.099548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1011053278304592000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             author_id  fishy_60_sum  fishy_60_count  fishy_40_sum  \\\n",
       "0           1009269193           0.0             221           0.0   \n",
       "1  1011053278304592000           0.0               0           0.0   \n",
       "\n",
       "   fishy_40_count  fishy_20_sum  fishy_20_count  C_0_sum  C_0_count  C_1_sum  \\\n",
       "0             221           0.0             221      0.0        221      1.0   \n",
       "1               0           0.0               0      0.0          0      0.0   \n",
       "\n",
       "   ...  fishy_share_20  C_0_share  C_1_share  C_2_share  C_3_share  C_4_share  \\\n",
       "0  ...             0.0        0.0   0.004525   0.208145   0.022727        0.0   \n",
       "1  ...             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "   C_5_share  C_6_share  C_7_share  C_8_share  \n",
       "0   0.330317   0.070588   0.081448   0.099548  \n",
       "1        NaN        NaN        NaN        NaN  \n",
       "\n",
       "[2 rows x 37 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"author_id\", \"fishy_60\", \"fishy_40\", \"fishy_20\"]\n",
    "fishy_user_count = tweets[tweets[\"retweeted\"] == False][cols + \\\n",
    "    list(nutrition_categories.values())]\\\n",
    "    .groupby(\"author_id\")\\\n",
    "    .agg([\"sum\", \"count\"])\n",
    "\n",
    "fishy_user_count[\"fishy_share_60\"] = fishy_user_count[\"fishy_60\"][\"sum\"] / \\\n",
    "                            fishy_user_count[\"fishy_60\"][\"count\"]\n",
    "fishy_user_count[\"fishy_share_40\"] = fishy_user_count[\"fishy_40\"][\"sum\"] / \\\n",
    "                            fishy_user_count[\"fishy_40\"][\"count\"]\n",
    "fishy_user_count[\"fishy_share_20\"] = fishy_user_count[\"fishy_20\"][\"sum\"] / \\\n",
    "                            fishy_user_count[\"fishy_20\"][\"count\"]\n",
    "\n",
    "for col in nutrition_categories.values():\n",
    "    fishy_user_count[f\"{col}_share\"] = fishy_user_count[col][\"sum\"] / \\\n",
    "                                       fishy_user_count[col][\"count\"]\n",
    "    \n",
    "# flatten the hierarchical indices\n",
    "fishy_user_count = fishy_user_count.reset_index()\n",
    "fishy_user_count.columns = ['_'.join(col).strip(\"_\") \\\n",
    "                            for col in fishy_user_count.columns.values]\n",
    "\n",
    "fishy_user_count.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e070f70f-4543-4858-be3e-cf2bded68125",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"fishy_share_60\", \"fishy_share_40\", \"fishy_share_20\", \"C_0_share\",\n",
    "        \"C_1_share\", \"C_2_share\", \"C_3_share\", \"C_4_share\", \"C_5_share\",\n",
    "        \"C_6_share\", \"C_7_share\", \"C_8_share\", \"author_id\"]\n",
    "users = pd.merge(users, fishy_user_count[cols], how=\"left\", left_on=\"author_id\",\n",
    "         right_on=\"author_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a1daea-e29c-4328-b1c2-05f8e18f3a0b",
   "metadata": {},
   "source": [
    "## Add average NewsGuard score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7b53b46-f10e-4ef0-93e9-e86325f82211",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_NG_scores = tweets[tweets[\"retweeted\"] == False][[\"author_id\", \"Score\"]]\\\n",
    "    .groupby(\"author_id\")\\\n",
    "    .mean()\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={\"Score\":\"NG_score_mean\"})\n",
    "users = pd.merge(users, average_NG_scores, how=\"left\", left_on=\"author_id\", right_on=\"author_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6307c56-adfd-4d83-b111-9bb2e8461225",
   "metadata": {},
   "source": [
    "## Add average accuracy & transparency score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d17fb525-9a04-4192-8d16-7729d7041660",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_accuracy_transparency = tweets[tweets[\"retweeted\"] == False][[\"author_id\", \"accuracy\", \"transparency\"]]\\\n",
    "    .groupby(\"author_id\")\\\n",
    "    .mean()\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={\"accuracy\":\"accuracy_mean\", \"transparency\":\"transparency_mean\"})\n",
    "users = pd.merge(users, average_accuracy_transparency, how=\"left\", left_on=\"author_id\", right_on=\"author_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977a4007-d097-44f5-a790-14bc31711dec",
   "metadata": {},
   "source": [
    "## Add share of unstrustworthy domains (independent list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40b07a89-36ca-4677-a73b-e853a1a20029",
   "metadata": {},
   "outputs": [],
   "source": [
    "unreliable_user_count = tweets[tweets[\"retweeted\"] == False][[\"author_id\", \"unreliable\"]]\\\n",
    "    .groupby(\"author_id\")\\\n",
    "    .agg([\"sum\", \"count\"])\n",
    "\n",
    "unreliable_user_count[\"unreliable_share\"] = unreliable_user_count[\"unreliable\"][\"sum\"] / \\\n",
    "                            unreliable_user_count[\"unreliable\"][\"count\"]\n",
    "    \n",
    "# flatten the hierarchical indices\n",
    "unreliable_user_count = unreliable_user_count.reset_index()\n",
    "unreliable_user_count.columns = ['_'.join(col).strip(\"_\") \\\n",
    "                            for col in unreliable_user_count.columns.values]\n",
    "\n",
    "users = pd.merge(users, unreliable_user_count[[\"author_id\", \"unreliable_share\"]],\n",
    "                 how=\"left\", left_on=\"author_id\", right_on=\"author_id\")\n",
    "del unreliable_user_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a79f8-ac7c-4e11-b20a-7e9103024582",
   "metadata": {},
   "source": [
    "## Add share of belief-speaking and truth-seeking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2be1701a-4889-4c8e-9232-8af3ffa360f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "honesty_tweets = tweets[tweets[\"retweeted\"] == False][[\"author_id\", \"belief\", \"truth\", \"created_at\"]]\\\n",
    "    .dropna(subset=[\"belief\", \"truth\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ca6315e-3f99-4205-a192-f3cb2f7d876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all honesty component tweets\n",
    "honesty_label_count = honesty_tweets[[\"author_id\", \"belief\", \"truth\"]]\\\n",
    "    .groupby(\"author_id\")\\\n",
    "    .agg([\"sum\", \"count\"])\n",
    "\n",
    "for col in [\"belief\", \"truth\"]:\n",
    "    honesty_label_count[f\"{col}_share\"] = honesty_label_count[col][\"sum\"] / \\\n",
    "    honesty_label_count[col][\"count\"]\n",
    "    \n",
    "honesty_label_count.columns = ['_'.join(col).strip(\"_\") \\\n",
    "                            for col in honesty_label_count.columns.values]\n",
    "honesty_label_count = honesty_label_count.reset_index()\n",
    "#honesty_label_count[\"author_id\"] = honesty_label_count[\"author_id\"].apply(match_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f78321f2-4713-4c72-8dad-7f0c1699df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "honesty_tweets = honesty_tweets.set_index(\"created_at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "425a85c9-0fcd-482c-8885-c6fc2ffe6db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only first 4 years\n",
    "honesty_label_count_first = honesty_tweets[honesty_tweets.index.year <= 2013]\\\n",
    "    .groupby(\"author_id\")\\\n",
    "    .agg([\"sum\", \"count\"])\n",
    "\n",
    "for col in [\"belief\", \"truth\"]:\n",
    "    honesty_label_count_first[f\"{col}_share_2010_to_2013\"] = honesty_label_count_first[col][\"sum\"] / \\\n",
    "    honesty_label_count_first[col][\"count\"]\n",
    "    \n",
    "honesty_label_count_first.columns = ['_'.join(col).strip(\"_\") \\\n",
    "                            for col in honesty_label_count_first.columns.values]\n",
    "honesty_label_count_first = honesty_label_count_first.reset_index()\n",
    "honesty_label_count_first[\"author_id\"] = honesty_label_count_first[\"author_id\"].apply(match_id)\n",
    "cols = [\"belief_sum\", \"belief_count\", \"truth_sum\", \"truth_count\"]\n",
    "honesty_label_count_first = honesty_label_count_first\\\n",
    "    .rename(columns={col:col + \"_2010_to_2013\" for col in cols}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6e0d952-d625-48e1-b7bf-25211b9cfda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only last 4 years\n",
    "honesty_label_count_last = honesty_tweets[honesty_tweets.index.year >= 2019]\\\n",
    "    [[\"author_id\", \"belief\", \"truth\"]]\\\n",
    "    .groupby(\"author_id\")\\\n",
    "    .agg([\"sum\", \"count\"])\n",
    "\n",
    "for col in [\"belief\", \"truth\"]:\n",
    "    honesty_label_count_last[f\"{col}_share_2019_to_2022\"] = honesty_label_count_last[col][\"sum\"] / \\\n",
    "    honesty_label_count_last[col][\"count\"]\n",
    "    \n",
    "honesty_label_count_last.columns = ['_'.join(col).strip(\"_\") \\\n",
    "                            for col in honesty_label_count_last.columns.values]\n",
    "honesty_label_count_last = honesty_label_count_last.reset_index()\n",
    "honesty_label_count_last[\"author_id\"] = honesty_label_count_last[\"author_id\"].apply(match_id)\n",
    "cols = [\"belief_sum\", \"belief_count\", \"truth_sum\", \"truth_count\"]\n",
    "honesty_label_count_last = honesty_label_count_last\\\n",
    "    .rename(columns={col:col + \"_2019_to_2022\" for col in cols}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "075e1209-11b5-46d5-bb87-beeb11251365",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users.merge(honesty_label_count[[\"author_id\", \"belief_share\", \n",
    "                    \"truth_share\"]], how=\"left\", left_on=\"author_id\", \n",
    "                    right_on=\"author_id\")\n",
    "del honesty_label_count\n",
    "\n",
    "users = users.merge(honesty_label_count_first[[\"author_id\", \"belief_share_2010_to_2013\",\n",
    "                        \"truth_share_2010_to_2013\"]], how=\"left\", left_on=\"author_id\", \n",
    "                         right_on=\"author_id\")\n",
    "del honesty_label_count_first\n",
    "\n",
    "users = users.merge(honesty_label_count_last[[\"author_id\",\"belief_share_2019_to_2022\",\n",
    "                        \"truth_share_2019_to_2022\"]], how=\"left\", left_on=\"author_id\", \n",
    "                         right_on=\"author_id\")\n",
    "del honesty_label_count_last"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624afd34-bcd0-434e-b756-61453b28276f",
   "metadata": {},
   "source": [
    "## Add share of neutral tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "edf0e7f8-08b7-4237-ba3c-6933667519d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "honesty_tweets = honesty_tweets.reset_index()\n",
    "neutral_count = honesty_tweets[honesty_tweets[[\"belief\", \"truth\"]]\\\n",
    "    .sum(axis=1) == 0][[\"author_id\", \"created_at\"]]\\\n",
    "    .groupby(\"author_id\")\\\n",
    "    .agg(\"count\")\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={\"created_at\":\"neutral_count\"})\n",
    "neutral_count[\"author_id\"] = neutral_count[\"author_id\"].apply(match_id)\n",
    "\n",
    "users = pd.merge(users, neutral_count, how=\"left\", left_on=\"author_id\",\n",
    "         right_on=\"author_id\").dropna(subset=[\"neutral_count\"])\n",
    "users[\"neutral_share\"] = users[\"neutral_count\"] / users[\"N_tweets\"]\n",
    "users = users.drop(columns=[\"neutral_count\"])\n",
    "del honesty_tweets\n",
    "del neutral_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4d5df9-59f6-42d8-85e7-8c3702603805",
   "metadata": {},
   "source": [
    "## Add ideology scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e77d54eb-c6c5-4ca6-9980-9dec16b0b719",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../data/twitter\"\n",
    "fname = \"govtrack-stats-{}-{}-ideology.csv\"\n",
    "ideology_scores = pd.DataFrame()\n",
    "for year in range(2013, 2021):\n",
    "    for chamber in [\"house\", \"senate\"]:\n",
    "        tmp = pd.read_csv(join(src, \"ideology_scores\",\n",
    "                               fname.format(year, chamber)))\n",
    "        tmp[\"year\"] = year\n",
    "        tmp[\"name\"] = tmp[\"name\"].apply(lambda x: x.replace(\"b'\", \"\"))\n",
    "        tmp[\"name\"] = tmp[\"name\"].apply(lambda x: x.replace(\"'\", \"\").lower())\n",
    "        ideology_scores = pd.concat([ideology_scores, tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4225918e-470e-4139-a980-d1c97587e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match politician Twitter account names to govtrack politician names\n",
    "\n",
    "# a single politician can have at maximum 8 entries for 8 different years\n",
    "# 2013 to 2020\n",
    "counts = ideology_scores[\"name\"].value_counts()\n",
    "unique_names = list(counts[counts <= 8].index)\n",
    "\n",
    "unique_scores = ideology_scores[ideology_scores[\"name\"].isin(unique_names)]\\\n",
    "    .sort_values(by=\"year\", ascending=False)\\\n",
    "    .drop_duplicates(subset=[\"name\"])\\\n",
    "    .set_index(\"name\")\n",
    "unique_names = list(set(unique_scores.index))\n",
    "\n",
    "def match_score(account_name):\n",
    "    '''Matches govtrack politician names to Twitter account names.'''\n",
    "    if account_name == account_name:\n",
    "        account_name = set(account_name.lower().split(\" \"))\n",
    "        for name in unique_names:\n",
    "            # hard matching: if the govtrack name string is completely included\n",
    "            # in the Twitter account name string, record a match\n",
    "            if name in account_name:\n",
    "                return unique_scores.loc[name][\"id\"]\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "users[\"ideology_score_id\"] = users[\"name\"].apply(match_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28e6f597-1240-4040-a814-8fab957ba0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add hand-matched missing scores\n",
    "src = \"../../data/twitter\"\n",
    "fname = \"missing_govtrack_ideology_scores.csv\"\n",
    "missing_scores = pd.read_csv(join(src, fname))\n",
    "missing_scores = {row[\"handle\"]:row[\"ideology_score_id\"] \\\n",
    "                  for i, row in missing_scores.iterrows()}\n",
    "\n",
    "# merge on the handle since this seems to be the most consistent index between\n",
    "# the two datasets\n",
    "users = users.set_index(\"handle\")\n",
    "for handle, score_id in missing_scores.items():\n",
    "    users.loc[handle, \"ideology_score_id\"] = score_id\n",
    "users = users.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "970b5966-4348-4e4a-b270-9f7b87177fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for many accounts, there is more than one ideology score since they were \n",
    "# active over many years. We calculate the mean, std and count of the ideology\n",
    "# score for each user and add this information to the user_df\n",
    "ideology_scores_agg = ideology_scores[[\"id\", \"ideology\"]]\\\n",
    "    .groupby(\"id\")\\\n",
    "    .agg([\"mean\", \"std\", \"count\"])\n",
    "ideology_scores_agg = ideology_scores_agg.reset_index()\n",
    "ideology_scores_agg.columns = ['_'.join(col).strip(\"_\") \\\n",
    "                            for col in ideology_scores_agg.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74895b07-eee3-4235-8818-e5c9e2f799e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users.merge(ideology_scores_agg, how=\"left\", \n",
    "                      left_on=\"ideology_score_id\", right_on=\"id\")\n",
    "del ideology_scores\n",
    "del ideology_scores_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e52b7e7-4487-4e38-a701-e572c43930dc",
   "metadata": {},
   "source": [
    "## Add Politifact scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc203c1d-6d46-413d-a1b5-2fb3a1e88b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../data\"\n",
    "fname = \"misinfo_score_politifact.csv\"\n",
    "pf_scores = pd.read_csv(join(src, fname), \n",
    "        usecols=[\"pf_score\", \"elite_account\"])\\\n",
    "    .rename(columns={\"elite_account\":\"handle\"})\n",
    "\n",
    "users = pd.merge(users, pf_scores, how=\"left\", left_on=\"handle\", right_on=\"handle\")\n",
    "del pf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4c0265-fecb-4048-bd50-802e4eeb3177",
   "metadata": {},
   "source": [
    "# Data exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f0fcc5e-1982-4323-801c-6614c8250d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = \"../../data/twitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "38c29338-6b28-41e7-b93c-c2ae005d9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls.to_csv(join(dst, \"US_URLs_2010-11-06_to_2022-03-16.csv.gzip\"),\n",
    "               index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d12e8d4-6651-4dc0-988a-fca8d50be901",
   "metadata": {},
   "outputs": [],
   "source": [
    "users.to_csv(join(dst, \"US_politician_twitter_account_stats_2010-11-06_to_2022-03-16.csv\"),\n",
    "               index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "17d62739-61ee-44a3-9076-9dc25bc4f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets[\"fishy_60\"] == 1].to_csv(join(dst, \"US_tweets_with_dodgy_links.csv\"),\n",
    "                                            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9fdd9ab7-7ac2-41f5-b606-c548cf99b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets[\"has_url\"] == True].to_csv(join(dst, \"US_tweets_with_urls.csv.gzip\"),\n",
    "                                            index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e2e6b858-14f8-40d0-bdc4-7ad842836fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv(join(dst, \"US_politician_tweets_2010-11-06_to_2022-03-16.csv.gzip\"),\n",
    "                        index=False, compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
